{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f00a010",
   "metadata": {},
   "source": [
    "# Description de données par un modèle affine via une descente de gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdda430",
   "metadata": {},
   "source": [
    "### *Enoncé*\n",
    "\n",
    "On prend comme cas d'études des données $\\boldsymbol{x}$ et $\\boldsymbol{y}$ liées par **une relation affine** de paramètres :\n",
    "* coefficient directeur : $\\boldsymbol{a = 2}$\n",
    "* ordonnée à l'origine :  $\\boldsymbol{b = 0.5}$\n",
    "\n",
    "polluées par un bruit centré obéissant à ***une loi normale d'écart-type 0.2***\n",
    "\n",
    "Créer une fonction `DGd` permettant, ***à partir d'un enregistrement de*** $\\boldsymbol{N}$ ***couples*** $\\boldsymbol{\\{\\,x_{[i]}\\,,\\,y_{[i]}\\,\\}}$, d'obtenir via l'algorithme de **descente de gradient** les paramètres de la relation qui lie ces 2 variables. Cette fonction prendra 5 paramètres d'entrée :\n",
    "* **x** : les données $x$\n",
    "* **y** : les données $y$\n",
    "* **Theta0** : valeur initiale du vecteur de paramètres *(défini comme un np.array)* \n",
    "* **lrate** : gain du gradient *(learning rate)*\n",
    "* **Nbmax** : nombre d'itérations maximales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "572e102c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da525dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta1 = 2\n",
    "theta2 = 0.5\n",
    "bruit = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcf12e1",
   "metadata": {},
   "source": [
    "On travaillera tout d'abord sur un premier enregistrement de **11 couples** $\\boldsymbol{\\{\\,x_{[i]}\\,,\\,y_{[i]}\\,\\}}$ avec $\\boldsymbol{x\\in[\\,0\\,,\\,1\\,]}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c902eddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0,1,11)\n",
    "y = theta1*x + theta2 + bruit*np.random.randn(len(x))\n",
    "\n",
    "plt.figure(0,figsize=(12,8))\n",
    "plt.scatter(x, y, label=\"données enregistrées\")\n",
    "plt.xlabel('variable x', fontsize=18)\n",
    "plt.ylabel('variable y', fontsize=18)\n",
    "plt.legend(fontsize=18)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "acdafd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DGd(x,y,Theta0,lrate,Nbmax):\n",
    "    N = len(x)\n",
    "    Theta = np.zeros((2,Nbmax+1))\n",
    "    # -> programmer l'algorithme de descente de gradient\n",
    "    # Les valeurs des paramètres Theta au fil des itérations sont à ranger dans le vecteur Theta\n",
    "    # Et la somme des carrés des erreurs à l'issue de l'algorithme sera notée coutfinal\n",
    "    \n",
    "    print('Les paramètres de la droite sont a = {:.4f} et b = {:.4f} ]'\n",
    "          .format(Theta[0,-1],Theta[1,-1]))\n",
    "    print('La somme des carrés des erreurs de modélisation est : {:.4f}'.format(coutfinal))\n",
    "    \n",
    "    plt.figure(1,figsize=(12,8))\n",
    "    plt.clf()\n",
    "    plt.scatter(x, y, c='b', label=\"données enregistrées\")\n",
    "    plt.plot(x,Theta[0,-1]*x + Theta[1,-1], c='r', label=\"données modélisées\")\n",
    "    plt.xlabel('variable x', fontsize=18)\n",
    "    plt.ylabel('variable y', fontsize=18)\n",
    "    plt.legend(fontsize=18)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(2,figsize=(12,8))\n",
    "    plt.clf()\n",
    "    plt.scatter(np.linspace(0,Nbmax,Nbmax+1), Theta[0,:], c = 'b', label = \"theta1 (coef directeur)\")\n",
    "    plt.scatter(np.linspace(0,Nbmax,Nbmax+1), Theta[1,:], c = 'c', label = \"theta2 (ordonnée origine)\")\n",
    "    plt.xlabel('itérations', fontsize=18)\n",
    "    plt.ylabel('paramètres', fontsize=18)\n",
    "    plt.legend(fontsize=18)\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a128349",
   "metadata": {},
   "outputs": [],
   "source": [
    "DGd(x,y,np.array([ 0 , 0 ]),0.05,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3d50fd",
   "metadata": {},
   "source": [
    "On considère ensuite un second enregistrement de **101 couples** $\\boldsymbol{\\{\\,x_{[i]}\\,,\\,y_{[i]}\\,\\}}$ avec toujours $\\boldsymbol{x\\in[\\,0\\,,\\,1\\,]}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "baebe73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0,1,101)\n",
    "y = theta1*x + theta2 + bruit*np.random.randn(len(x))\n",
    "\n",
    "plt.figure(0,figsize=(12,8))\n",
    "plt.clf()\n",
    "plt.scatter(x, y, label=\"données enregistrées\")\n",
    "plt.xlabel('variable x', fontsize=18)\n",
    "plt.ylabel('variable y', fontsize=18)\n",
    "plt.legend(fontsize=18)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4b82f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DGd(x,y,np.array([ 0 , 0 ]),0.005,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230564cf",
   "metadata": {},
   "source": [
    "Pour traiter le second jeu de données expérimentales, il a fallu **diviser le gain d'apprentissage par 10**  \n",
    "$\\boldsymbol{\\rightarrow}$ **cela est tout à fait normal**  \n",
    "- puisqu'il y a 10 fois plus de données\n",
    "- la fonction coût *(qui est la somme des carrés des erreurs)* aura une amplitude 10 fois plus importante\n",
    "- et donc il en sera de même pour son gradient *(puisqu'il indique la variation de la fonction coût)*  \n",
    "\n",
    "**En conséquence, le gain d'apprentissage doit toujours être normalisé par le nombre de données**, de façon qu'on puisse traiter n'importe quelle taille de jeu de données sans qu'il soit nécessaire de retoucher le gain à chaque fois  \n",
    "On prendra également comme crière de performance, non pas la somme des carrés des erreurs, mais le carré de l'erreur moyenne sur une donnée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "15d56cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DGd2(x,y,Theta0,lrate,Nbmax):\n",
    "    N = len(x)\n",
    "    Theta = np.zeros((2,Nbmax+1))\n",
    "    # -> apporter les modifications demandées à votre algorithme de descente de gradient\n",
    "    \n",
    "    print('Les paramètres de la droite sont a = {:.4f} et b = {:.4f} ]'\n",
    "          .format(Theta[0,-1],Theta[1,-1]))\n",
    "    print('Erreur de modélisation moyenne au carré : {:.4f}'.format(coutfinal)\n",
    "    \n",
    "    plt.figure(1,figsize=(12,8))\n",
    "    plt.clf()\n",
    "    plt.scatter(x, y, c='b', label=\"données enregistrées\")\n",
    "    plt.plot(x,Theta[0,-1]*x + Theta[1,-1], c='r', label=\"données modélisées\")\n",
    "    plt.xlabel('variable x', fontsize=18)\n",
    "    plt.ylabel('variable y', fontsize=18)\n",
    "    plt.legend(fontsize=18)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(2,figsize=(12,8))\n",
    "    plt.clf()\n",
    "    plt.scatter(np.linspace(0,Nbmax,Nbmax+1), Theta[0,:], c = 'b', label = \"theta1 (coef directeur)\")\n",
    "    plt.scatter(np.linspace(0,Nbmax,Nbmax+1), Theta[1,:], c = 'c', label = \"theta2 (ordonnée origine)\")\n",
    "    plt.xlabel('itérations', fontsize=18)\n",
    "    plt.ylabel('paramètres', fontsize=18)\n",
    "    plt.legend(fontsize=18)\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ff0b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DGd2(x,y,np.array([ 0 , 0 ]),0.5,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80617c7",
   "metadata": {},
   "source": [
    "On peut ainsi considérer un troisième enregistrement de **1001 couples** $\\boldsymbol{\\{\\,x_{[i]}\\,,\\,y_{[i]}\\,\\}}$ avec  $\\boldsymbol{x\\in[\\,0\\,,\\,1\\,]}$ sans avoir à retoucher au gain d'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f47fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0,1,1001)\n",
    "y = theta1*x + theta2 + bruit*np.random.randn(len(x))\n",
    "\n",
    "plt.figure(0,figsize=(12,8))\n",
    "plt.clf()\n",
    "plt.scatter(x, y, label=\"données enregistrées\")\n",
    "plt.xlabel('variable x', fontsize=18)\n",
    "plt.ylabel('variable y', fontsize=18)\n",
    "plt.legend(fontsize=18)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59ad2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DGd2(x,y,np.array([ 0 , 0 ]),0.5,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15558d1",
   "metadata": {},
   "source": [
    "On considère maintenant un quatrième et dernier enregistrement de **1001 couples** $\\boldsymbol{\\{\\,x_{[i]}\\,,\\,y_{[i]}\\,\\}}$ avec cette fois $\\boldsymbol{x\\in[\\,0\\,,\\,10\\,]}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a686fcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0,10,1001)\n",
    "y = theta1*x + theta2 + bruit*np.random.randn(len(x))\n",
    "\n",
    "plt.figure(0,figsize=(12,8))\n",
    "plt.clf()\n",
    "plt.scatter(x, y, label=\"données enregistrées\")\n",
    "plt.xlabel('variable x', fontsize=18)\n",
    "plt.ylabel('variable y', fontsize=18)\n",
    "plt.legend(fontsize=18)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ae906a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DGd2(x,y,np.array([ 0 , 0 ]),0.005,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3ca5b8",
   "metadata": {},
   "source": [
    "Une nouvelle fois, il a fallu réduire largement le gain d'apprentissage pour obtenir la convergence des paramètres  \n",
    "$\\boldsymbol{\\rightarrow}$  **encore une fois, c'est tout à fait normal**  \n",
    "- le gradient de la fonction coût par rapport au paramètre $a$ est la variable $x$\n",
    "- la variable $x$ prenant cette fois des valeurs beaucoup plus importantes *(puisque $x$ monte jusque 10 désormais)*, le gradient va augmenter, ce qui oblige à baisser le gain d'apprentissage pour ne pas faire de trop grand pas\n",
    "\n",
    "Pour ne pas être tributaire de la valeur brute des paramètres, il est **sain de les normaliser entre 0 et** $\\boldsymbol{\\pm}1$ avant de lancer la modélisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e2a7dba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DGd3(x,y,Theta0,lrate,Nbmax):\n",
    "    N = len(x)\n",
    "    Theta = np.zeros((2,Nbmax+1))\n",
    "    # -> apporter les modifications demandées à votre algorithme de descente de gradient\n",
    "    \n",
    "    print('Les paramètres de la droite sont a = {:.4f} et b = {:.4f}'\n",
    "          .format(a,b))\n",
    "    print('Erreur de modélisation moyenne au carré : {:.4f}'.format(coutfinal))\n",
    "    \n",
    "    plt.figure(1,figsize=(12,8))\n",
    "    plt.clf()\n",
    "    plt.scatter(x, y, c='b', label=\"données enregistrées\")\n",
    "    plt.plot(x,a*x + b, c='r', label=\"données modélisées\")\n",
    "    plt.xlabel('variable x', fontsize=18)\n",
    "    plt.ylabel('variable y', fontsize=18)\n",
    "    plt.legend(fontsize=18)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(2,figsize=(12,8))\n",
    "    plt.clf()\n",
    "    plt.scatter(np.linspace(0,Nbmax,Nbmax+1), Theta[0,:]*My/Mx, c = 'b', label = \"theta1 (coef directeur)\")\n",
    "    plt.scatter(np.linspace(0,Nbmax,Nbmax+1), Theta[1,:]*My, c = 'c', label = \"theta2 (ordonnée origine)\")\n",
    "    plt.xlabel('itérations', fontsize=18)\n",
    "    plt.ylabel('paramètres', fontsize=18)\n",
    "    plt.legend(fontsize=18)\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404ed10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DGd3(x,y,np.array([ 0 , 0 ]),0.5,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dd3cfc",
   "metadata": {},
   "source": [
    "Pour alléger le coût du calcul du gradient à chaque itération, on peut se tourner vers **une descente de gradient stochastique**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dee3f37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DGd4(x,y,Theta0,lrate,Nbmax,batch):\n",
    "    \n",
    "    # Pour cette dernière version de l'algorithme de descente de gradient\n",
    "    # il est demandé de bien comprendre le sens des différentes variables\n",
    "    # et des nouvelles instructions qui ont été introduites\n",
    "    \n",
    "    Mx = np.max(x)\n",
    "    xn = x / Mx\n",
    "    My = np.max(y)\n",
    "    yn = y / My\n",
    "    \n",
    "    N = len(x)\n",
    "    Nbatch = int(N//batch)\n",
    "    epoch = int(Nbmax//Nbatch+1)\n",
    "    iterations = epoch*Nbatch\n",
    "    Theta = np.zeros((2,iterations+1))\n",
    "    Theta[:,0] = Theta0\n",
    "    \n",
    "    print('La taille du batch est :',batch)\n",
    "    print('donc le nombre de batch pour couvrir le jeu de données est :',Nbatch)\n",
    "    print('Le nombre de passe pour au moins atteindre le nombre d\\'itérations max est :',epoch)\n",
    "    print('ce qui conduit au total à',iterations,'itérations où l\\'on manipule des matrices de dimension : ',\n",
    "          batch,'x 2')\n",
    "    \n",
    "    Data = np.hstack((xn.reshape(-1,1),yn.reshape(-1,1)))\n",
    "    \n",
    "    iter = 0\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        np.random.shuffle(Data)\n",
    "        for j in range(Nbatch):\n",
    "            V = Data[batch*j:batch*(j+1),1] - ( Theta[0,iter]*Data[batch*j:batch*(j+1),0] + Theta[1,iter] )\n",
    "            M = np.hstack( ( -Data[batch*j:batch*(j+1),0].reshape(-1,1) , -np.ones((batch,1)) ))\n",
    "            Theta[:,iter+1] = Theta[:,iter] - (lrate/batch)*2*M.T.dot(V)\n",
    "            iter += 1\n",
    "            \n",
    "    a = Theta[0,-1]*My/Mx\n",
    "    b = Theta[1,-1]*My\n",
    "            \n",
    "    V = y - ( a*x + b )\n",
    "    print('Les paramètres de la droite sont a = {:.4f} et b = {:.4f}'\n",
    "          .format(a,b))\n",
    "    print('Erreur de modélisation moyenne au carré : {:.4f}'.format((1/N)*V.T.dot(V)))\n",
    "    \n",
    "    plt.figure(1,figsize=(12,8))\n",
    "    plt.clf()\n",
    "    plt.scatter(x, y, c='b', label=\"données enregistrées\")\n",
    "    plt.plot(x,a*x + b, c='r', label=\"données modélisées\")\n",
    "    plt.xlabel('variable x', fontsize=18)\n",
    "    plt.ylabel('variable y', fontsize=18)\n",
    "    plt.legend(fontsize=18)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(2,figsize=(12,8))\n",
    "    plt.clf()\n",
    "    plt.scatter(np.linspace(0,iterations,iterations+1), Theta[0,:]*My/Mx, c = 'b', label = \"theta1 (coef directeur)\")\n",
    "    plt.scatter(np.linspace(0,iterations,iterations+1), Theta[1,:]*My, c = 'c', label = \"theta2 (ordonnée origine)\")\n",
    "    plt.xlabel('itérations', fontsize=18)\n",
    "    plt.ylabel('paramètres', fontsize=18)\n",
    "    plt.legend(fontsize=18)\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b8e573",
   "metadata": {},
   "source": [
    "Bien entendu, si ***la taille du batch est égale à la taille des données***, alors les 2 algorithmes de descente de gradient et de descente de gradient stochastique sont identiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0f97dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DGd4(x,y,np.array([ 0 , 0 ]),0.5,100,len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94620be1",
   "metadata": {},
   "source": [
    "Ensuite, ***plus on réduit la taille du batch***, moins le coût calculatoire par itération est important, mais plus l'approximation du gradient est bruitée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8358a593",
   "metadata": {},
   "outputs": [],
   "source": [
    "DGd4(x,y,np.array([ 0 , 0 ]),0.5,100,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a40954b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DGd4(x,y,np.array([ 0 , 0 ]),0.5,100,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8a1ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DGd4(x,y,np.array([ 0 , 0 ]),0.5,100,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c90314",
   "metadata": {},
   "outputs": [],
   "source": [
    "DGd4(x,y,np.array([ 0 , 0 ]),0.5,100,1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
